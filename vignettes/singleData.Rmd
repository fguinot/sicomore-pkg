---
title: "Illustration of getHierLevel function for finding and selecting relevant groups of variable"
author: "SIComORe team"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
link-citations: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{sicomoreSingleData}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  screenshot.force = FALSE, 
  echo = TRUE,
  rows.print = 5,
  message = FALSE, 
  warning = FALSE)
```

## Preliminaries

This vignette illustrates SICOMORE for analysis a single data set with correlated predictors related to a phenotype. 

```{r message=FALSE, warning=FALSE, include=FALSE}
set.seed(1234)
library(microbenchmark)
library(Matrix)
library(cowplot)
library(ggplot2)
library(magrittr)
library(mvtnorm)
library(SIComORe)
```

## Data generation

### Input matrix of predictors

Now, let us to draw some data: we consider 200 variables spreaded in 3 groups of 30 variables + 110 groups with a single variable. Only groups 1 and 2 will be relevant for the phenotype

```{r}
p <- 200  # the number of features
K <- 113  # number of groups 
## 2 relevants groups with 30 variables
## 1 irrelevant group with 30 variables
## 110 irrelevant groups with a single variable
grp.size <- c(30, 30, 30, rep(1,110))
```

We draw 100 observation of a Gaussian vector the covariance of which has a block structure faithfull to the original grouping of the variables.

```{r}
## covariance defined blockwise
n <- 100
rhos <- runif(K,.5,.95) # correlation within groups
Sigma <- bdiag(lapply(1:K, function(k) return(matrix(rhos[k],grp.size[k],grp.size[k]))))
diag(Sigma) <- 1
grp <- rep(1:length(grp.size), grp.size)
X <- Sigma %>%
  as.matrix() %>%
  rmvnorm(n, sigma=.) %>%
  scale()
```

The matrix of empirical correlation is rather convincing:

```{r, include=TRUE, fig.width=7}
X %>% 
  cor() %>%
  Matrix %>%
  image(main="cor(X)")
```

### Phenotype/Response vector

The phenotype will be a linear combinason of the *compressed* version of the predictors. Here, we use the mean to compressed the variable which are in the same group:

```{r}
X.comp <- t(rowsum(t(X), grp)/tabulate(grp))
```

The vector of regression parameters is sparse, chosen such that the first two group are predictive:

```{r}
dim.theta <- ncol(X.comp)
theta <- rep(0, dim.theta)
theta[c(1,2)] <- runif(2, min=5, max=10) ## simple effects on the first two groups
```

Finally, the response vector is drawn from a linear model. The level of noise is such that the $R^2$ is approximately $0.75$.

```{r}
sigma <- 5
epsilon <- rnorm(n) * sigma
epsilon.test <- rnorm(n) * sigma
y <- X.comp %*% theta + epsilon
r2 <- 1-sum(epsilon^2) / sum((y-mean(y))^2)
r2
```

## Adjusting a single hierarchy

Now, we try to recover the correct level of compression only from the original input matrix and the response vector. We test all the level of a hierarchy obtained by a WARD hierarchical clustering. The grouping is rather obvious in this case: 

### Retreiving a hierarchy 

```{r, fig.width=7}
hierarchy <- X %>% 
  scale() %>%
  t() %>%
  dist() %>%
  hclust(method="ward.D2")
plot(hierarchy)
```

### Variable selection along the hierarchy

We try the 3 available options in our package for simultaneously finding and selecting the groups of variables related to the phenotype:

```{r}
out.rho  <- getHierLevel(X, y, hierarchy, choice="lambda.1se", selection="rho-sicomore", mc.cores=NULL)
out.mlgl  <- getHierLevel(X, y, hierarchy, choice="lambda.1se", selection="mlgl", mc.cores=NULL)
out.sicomore <- getHierLevel(X, y, hierarchy, choice="lambda.1se", selection="sicomore", mc.cores=NULL)
```

The three models show comparable estimated prediction error on the best level of the hierarchy:

```{r, fig.width=7, warning=FALSE}
all.cv <- rbind(cbind(out.rho$cv.error , method="rho-sicomore"),
                cbind(out.sicomore$cv.error, method="sicomore"),
                cbind(out.mlgl$cv.error , method="mlgl"))
all.cv %>% 
  ggplot(aes(x=lambda, y=mean, colour=method, group=method)) + 
  geom_smooth(aes(ymin=mean-sd, ymax=mean+sd), stat="identity") + 
   labs(y = "Mean cross-validation error", x = "Lambda") +
  coord_trans(x="log")
```

In terms of variables selections, 

```{r}
out.mlgl$getGrp()
out.rho$getGrp()
out.sicomore$getGrp()
```

## Comparison of computation time between the 3 options:

```{r, message=FALSE, warning=FALSE, include=TRUE}
RHO      <- expression(getHierLevel(X, y, hierarchy, choice="lambda.1se", selection="rho-sicomore", mc.cores=NULL))
SICOMORE <- expression(getHierLevel(X, y, hierarchy, choice="lambda.1se", selection="sicomore", mc.cores=NULL))
MLGL     <- expression(getHierLevel(X, y, hierarchy, choice="lambda.1se", selection="mlgl", mc.cores=NULL))
bench    <- microbenchmark(eval(RHO), eval(SICOMORE) , eval(MLGL), times = 5)
```

```{r,fig.width=7}
autoplot(bench)
```
